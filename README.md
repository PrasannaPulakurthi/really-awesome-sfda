# Awesome Source‑Free Domain Adaptation (SFDA) [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

Curated resources for Source‑Free Domain Adaptation (SFDA): methods that adapt using only a trained source model (no source data). Includes papers, code, datasets, benchmarks, and tutorials.
<!--lint disable awesome-github repo-url -->

## Contents
- [Papers](#papers)
- [Datasets & Benchmarks](#datasets--benchmarks)
- [Libraries & Tooling](#libraries--tooling)
- [Tutorials & Talks](#tutorials--talks)

## Papers
### 2024
- A Comprehensive Survey on Source-Free Domain Adaptation (TPAMI 2024) [[Paper](https://ieeexplore.ieee.org/document/10533539)] [[arXiv](https://arxiv.org/abs/2302.11803)]
- GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain Adaptation (TPAMI 2024) [[Paper](https://ieeexplore.ieee.org/document/10405692)] [[arXiv](https://arxiv.org/abs/2307.08740)] [[Code](https://github.com/Jerry-Luo-98/GALA)]
- Neighborhood-Aware Mutual Information Maximization for SFDA (TMM 2024) [[Paper](https://ieeexplore.ieee.org/document/10603809)] [[arXiv](https://arxiv.org/abs/2403.18239)]
- ReCLIP: Refine Contrastive Language Image Pre-Training with SFDA (WACV 2024) [[Paper](https://openaccess.thecvf.com/content/WACV2024/html/Yao_ReCLIP_Refine_Contrastive_Language_Image_Pre-Training_with_Source-Free_Domain_Adaptation_WACV_2024_paper.html)] [[arXiv](https://arxiv.org/abs/2310.10893)] [[Code](https://github.com/parasol-team/reclip)]
- SF(DA)²: Source-free Domain Adaptation Through the Lens of Data Augmentation (ICLR 2024) [[Paper](https://openreview.net/forum?id=4WJ1X0XyBI)] [[arXiv](https://arxiv.org/abs/2312.08566)] [[Code](https://github.com/shinyflight/SFDA2)]
- SepRep-Net: Multi-source Free Domain Adaptation via Model Separation and Reparameterization (arXiv 2024) [[Paper](https://arxiv.org/abs/2402.08249)]
- SFDA for RGB-D Semantic Segmentation with Vision Transformers (arXiv 2024) [[Paper](https://arxiv.org/abs/2406.19533)]
- SFDA with Diffusion-Guided Source Data Generation (arXiv 2024) [[Paper](https://arxiv.org/abs/2401.12047)]
- SFDA with Frozen Multimodal Foundation Model (CVPR 2024) [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Tang_Source-Free_Domain_Adaptation_with_Frozen_Multimodal_Foundation_Model_CVPR_2024_paper.html)] [[arXiv](https://arxiv.org/abs/2403.11066)] [[Code (repo incl. DIFO)](https://github.com/tntek/source-free-domain-adaptation)]
- Understanding and Improving SFDA from a Theoretical Perspective (CVPR 2024) [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Kawasaki_Understanding_and_Improving_Source-free_Domain_Adaptation_from_a_Theoretical_Perspective_CVPR_2024_paper.html)] [[arXiv](https://arxiv.org/abs/2403.15957)] [[Code](https://github.com/nttcslab/improved_sfda)]
- Visually Source-Free Domain Adaptation via Adversarial Style Matching (TIP 2024) [[Paper](https://ieeexplore.ieee.org/document/10424442)]

## Datasets & Benchmarks
- DomainNet — multi‑domain classification benchmark.
- Office‑31 / Office‑Home — classic DA suites, widely reused in SFDA.
- VisDA‑2017 — synthetic→real; used by SFDA methods.

## Libraries & Tooling
- Add libraries and tooling here.

## Tutorials & Talks
- Add tutorials and talks here.

## Contributing
Please read [CONTRIBUTING.md](CONTRIBUTING.md). Use this entry format:

```
- Title (Venue/Year) [[paper](https://example.com/paper)] [[code](https://example.com/code)] [[project](https://example.com/site)]
```
